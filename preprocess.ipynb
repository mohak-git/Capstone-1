{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde4b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from autocorrect import Speller\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4df648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load dataset\n",
    "data_path = \"data/En-Ba-Dataset(20k_4)/dataset.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Ensure dataset has Sentence and Label\n",
    "df.columns = [\"Sentence\", \"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081960fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define preprocessing functions\n",
    "\n",
    "\n",
    "# Lowercasing\n",
    "def to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "# Remove HTML Tags\n",
    "def remove_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "\n",
    "# Remove URLs\n",
    "def remove_urls(text):\n",
    "    return re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "\n",
    "# Remove punctuation\n",
    "def remove_punct(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "\n",
    "# Chat word treatment (custom shortforms → full words)\n",
    "chat_dict = {\n",
    "    \"u\": \"you\",\n",
    "    \"ur\": \"your\",\n",
    "    \"r\": \"are\",\n",
    "    \"pls\": \"please\",\n",
    "    \"plz\": \"please\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"tnx\": \"thanks\",\n",
    "    \"luv\": \"love\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"idk\": \"i do not know\",\n",
    "}\n",
    "\n",
    "\n",
    "def chat_word_treatment(text):\n",
    "    tokens = text.split()\n",
    "    new_tokens = [chat_dict.get(w, w) for w in tokens]\n",
    "    return \" \".join(new_tokens)\n",
    "\n",
    "\n",
    "# Spelling correction\n",
    "spell = Speller(lang=\"en\")\n",
    "\n",
    "\n",
    "def correct_spelling(text):\n",
    "    tokens = text.split()\n",
    "    corrected = [spell(w) for w in tokens]\n",
    "    return \" \".join(corrected)\n",
    "\n",
    "\n",
    "# Handling emojis → convert to text\n",
    "def handle_emojis(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    tokens = tokenize(text)\n",
    "    lemmatized = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "    return \" \".join(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253108e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Full pipeline\n",
    "def preprocess(text):\n",
    "    text = to_lower(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_punct(text)\n",
    "    text = chat_word_treatment(text)\n",
    "    text = handle_emojis(text)\n",
    "    text = lemmatize(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "df[\"Sentence\"] = df[\"Sentence\"].astype(str).apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674a8cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Save processed dataset\n",
    "output_path = \"data/En-Ba-Dataset(20k_4)/dataset_cleaned.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Show sample\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
